{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Poetic Text Generator using RNN with LSTM cells#"
      ],
      "metadata": {
        "id": "AURsZU75DDhh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Aim##\n",
        "To use LSTM neural networks (Long-Short-Term Memory) in order to teach our computer to write texts like Shakespeare. The way we do that is by training a recurrent neural network (RNN) model with LSTM cells to generate text similar to Shakespeare's writing style."
      ],
      "metadata": {
        "id": "Q7uiOWJcCBFa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " The program you see below uses TensorFlow to download the \"shakespeare.txt\" file, preprocesses the text data, creates input and target sequences, defines an RNN model with LSTM cells, trains the model using the dataset, and finally generates new text using the trained model."
      ],
      "metadata": {
        "id": "XP5EMUwrBw-1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We import the necessary libraries:"
      ],
      "metadata": {
        "id": "JQU_H_GP8dng"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gMx2K_O28Vg7"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Activation\n",
        "from tensorflow.keras.optimizers import RMSprop"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define the URL where the Shakespeare text file is located and use the requests library to download the file:"
      ],
      "metadata": {
        "id": "5ZxFrtW78knp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filepath = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "\n",
        "text = open(filepath, 'rb').read().decode(encoding='utf-8').lower()"
      ],
      "metadata": {
        "id": "OX7WCkElCKwL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We know that a neural network canâ€™t take in the raw string data, so we need to encode it by assigning numbers to each character.\n",
        "We preprocess the text by creating a vocabulary of unique characters and mapping each character to an index:"
      ],
      "metadata": {
        "id": "C_kZ-L7Y89fv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create input sequences and target sequences by splitting the text into chunks of a defined sequence length:"
      ],
      "metadata": {
        "id": "aSrkcU789CV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "characters = sorted(set(text))\n",
        "\n",
        "char_to_index = dict((c, i) for i, c in enumerate(characters))\n",
        "index_to_char = dict((i, c) for i, c in enumerate(characters))\n",
        "\n",
        "SEQ_LENGTH = 100\n",
        "STEP_SIZE = 3\n",
        "\n",
        "sentences = []\n",
        "next_char = []\n",
        "\n",
        "for i in range(0, len(text) - SEQ_LENGTH, STEP_SIZE):\n",
        "    sentences.append(text[i: i + SEQ_LENGTH])\n",
        "    next_char.append(text[i + SEQ_LENGTH])\n",
        "\n",
        "x = np.zeros((len(sentences), SEQ_LENGTH,\n",
        "              len(characters)), dtype=np.bool)\n",
        "y = np.zeros((len(sentences),\n",
        "              len(characters)), dtype=np.bool)\n",
        "\n",
        "for i, satz in enumerate(sentences):\n",
        "    for t, char in enumerate(satz):\n",
        "        x[i, t, char_to_index[char]] = 1\n",
        "    y[i, char_to_index[next_char[i]]] = 1"
      ],
      "metadata": {
        "id": "maC4zktNCWqN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37b37c83-5f98-4657-d686-95481d35d471"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-6b388c12ad21>:17: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  len(characters)), dtype=np.bool)\n",
            "<ipython-input-5-6b388c12ad21>:19: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  len(characters)), dtype=np.bool)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We train the model using the prepared dataset and also compile the model.\n",
        "We define the architecture of the RNN model using LSTM cells:"
      ],
      "metadata": {
        "id": "19II9jI-9mRQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(128,\n",
        "               input_shape=(SEQ_LENGTH,\n",
        "                            len(characters))))\n",
        "model.add(Dense(len(characters)))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=RMSprop(learning_rate=0.01))\n",
        "\n",
        "model.fit(x, y, batch_size=256, epochs=5)"
      ],
      "metadata": {
        "id": "6e8yRIbECbrY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05bdd939-b701-4c40-f2da-35a8cfc8d475"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "  56/1453 [>.............................] - ETA: 13:02 - loss: 3.0133"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper function for randomnly sampling text."
      ],
      "metadata": {
        "id": "Wapd9shupCss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ],
      "metadata": {
        "id": "QOWYcyLqChDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining a utility function to generate new and random text using the trained model:"
      ],
      "metadata": {
        "id": "Nq1j-YSBniq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(length, temperature):\n",
        "    start_index = random.randint(0, len(text) - SEQ_LENGTH - 1)\n",
        "    generated = ''\n",
        "    sentence = text[start_index: start_index + SEQ_LENGTH]\n",
        "    generated += sentence\n",
        "    for i in range(length):\n",
        "        x_predictions = np.zeros((1, SEQ_LENGTH, len(characters)))\n",
        "        for t, char in enumerate(sentence):\n",
        "            x_predictions[0, t, char_to_index[char]] = 1\n",
        "\n",
        "        predictions = model.predict(x_predictions, verbose=0)[0]\n",
        "        next_index = sample(predictions,\n",
        "                                 temperature)\n",
        "        next_character = index_to_char[next_index]\n",
        "\n",
        "        generated += next_character\n",
        "        sentence = sentence[1:] + next_character\n",
        "    return generated"
      ],
      "metadata": {
        "id": "5eTuIo5kCkeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generating texts with varying temperature parameter for its generation."
      ],
      "metadata": {
        "id": "hG9yCdPhoV-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"----------0.2--------\")\n",
        "print(generate_text(300, 0.2))\n",
        "print(\"----------0.4--------\")\n",
        "print(generate_text(300, 0.4))\n",
        "print(\"----------0.5--------\")\n",
        "print(generate_text(300, 0.5))\n",
        "print(\"----------0.6--------\")\n",
        "print(generate_text(300, 0.6))\n",
        "print(\"----------0.7--------\")\n",
        "print(generate_text(300, 0.7))\n",
        "print(\"----------0.8--------\")\n",
        "print(generate_text(300, 0.8))"
      ],
      "metadata": {
        "id": "NtF1dxic8luX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}